---
title: "Introduction to Principal Component Analysis, part 1"
author: "Nurlan Dauletbayev"
date: "2023-11-05"
output: html_document
---
# A. Package installation and custom function
These two packages will be needed for an interactive 3D plot. Just copy and paste the code into your R script. The concept of R packages and their use will be discussed on a later occassion.
```{r}
if (!require("rgl")) {
  install.packages("rgl", dependencies = TRUE)
  library(rgl)
}

if (!require("car")) {
  install.packages("car", dependencies = TRUE)
  library(car)
}
```
This custom function will be needed at one of the steps to show the correlation coefficient. Please copy the code and use it as above.
```{r}
panel.cor <- function(x, y, ...)
{
  par(usr = c(0, 1, 0, 1))
  txt <- as.character(format(cor(x, y), digits = 3))
  text(0.5, 0.5, txt,  cex = 2* abs(cor(x, y)))
}
```
# Principal Component Analysis (PCA) using toy examples
PCA will first be explained on two- and three-dimensional toy examples. We will define three vectors that will later be combined into matrices of two or three variables.
```{r}
x_vect <- seq(2, 5, 0.1)
print(x_vect)
y_vect <- sin(x_vect)
print(y_vect)
# to introduce more variability into the second vector, R function "jitter()" 
# will be used; if you are curious, try to change the extent of variability by
# changing the numerical value in the argument "amount =" 
set.seed(42)
# the use of function (set.see()) is needed to ensure reproducibility of
# function "jitter()"; this will be discussed in subsequent classes. "42" is 
# just an arbitrary number
y1_vect <- jitter(y_vect, factor = 1, amount = 0.8)
print(y1_vect)
# the third vector will be derived from the second vector, and some variabiliy
# will be introduced by the use of function "jitter()"
set.seed(42)
z_vect <- jitter(log10(y1_vect + 12), factor = 1, amount = 0.8)
print(z_vect)
```
This will create a matrix (and then, a data frame) from the above vectors. Please note the use of functions "cbind()", "as.data.frame()", and "plot()":
```{r}
Example.1 <- cbind(x_vect, y1_vect)
class(Example.1)
df_Example.1 <- as.data.frame(Example.1)
class(df_Example.1)
plot(df_Example.1)
```

The following script introduces additional descriptive visualization (boxplot, overimposed by individual data points), using the functions "boxplot()" and "stripchart()". Please note that both lines of this code chunk should be run together (to avoid potential errors).
```{r}
boxplot(df_Example.1)
stripchart(df_Example.1, method = "jitter", vertical = TRUE,
           pch = 21, add = TRUE)
```

This boxplot demonstrates that the values of both variables differ in the scale (y axis). Also the data dispersion is different. To address this, the matrix will be subjected to Z-normalization using function "scale()".
```{r}
Example.1_sc <- scale(Example.1, center = TRUE, scale = TRUE)
print(Example.1_sc)
df_Example.1_sc <- as.data.frame(Example.1_sc) # this data frame will be needed
                                               # for building a boxplot
print(df_Example.1_sc,
      xlim = c(-4, 4), ylim = c(-2, 2))
plot(df_Example.1_sc)
boxplot(df_Example.1_sc)
stripchart(df_Example.1_sc, method = "jitter", vertical = TRUE,
           pch = 21, add = TRUE)

```

PCA can be done using a covariance or correlation matrix. The next section will show PCA based on a correlation matrix, which is easier to understand in the beginning. 

The first assumption of PCA is that some (or all) variables in the analyzed matrix correlate with other variable to a certain degree. If this correlation (positive or negative) exists, then the correlated variables can be collapsed and combined (entirely or in part) into artificial vectors called "principal components". This step leads to a great simplification of the analyzed matrix (that is, in reduction of dimensionality). 

The next assumption of PCA is that some variables only negligibly contribute to the total variation of the matrix and can therefore be discarded from the analysis. This is a second how PCA can reduce the dimensionality.

We will next assess the "Example.1" matrix for the presence of correlated variables.
```{r}
print(cor(Example.1_sc, method = "pearson")) # calculates a correlation matrix
# the above custom function "panel.cor" shows correlation coefficients
# in this plot
pairs(Example.1_sc,
      upper.panel = panel.cor)
```

This matrix demonstrates a strong negative correlation between variables, justing the use of PCA. One of the steps of dimensionality reduction by PCA is matrix decomposition. This can be done, among other methods, by the eigenvalue decomposition (shown below) or the singular value decomposition (shown subsequently). Please refer to my presentation for a brief introduction into either decomposition. The eigen decomposition requires some transformation of the original matrix, in particular, generation of a square matrix from the original matrix. Such square matrix is, for example, the correlation matrix used in one of the previous steps above.
```{r}
# calculates and demonstrates the correlation matrix
cor_Example.1 <- cor(Example.1_sc, method = "pearson")
print(cor_Example.1)               
eigen_Example.1 <- eigen(cor_Example.1)
print(eigen_Example.1)
class(eigen_Example.1) # shows class of the object as "eigen"
# function "str()" shows structure of the object; "values" and "vectors"
# will be needed in the next step
str(eigen_Example.1) 
# extracts the "values"
EigenvaluesExample.1 <- eigen_Example.1$values
print(EigenvaluesExample.1)
# extracts the "vectors"
EigenvectorsExample.1 <- eigen_Example.1$vectors
print(EigenvectorsExample.1)
# assigns proper column names
colnames(EigenvectorsExample.1) <- c("PC1", "PC2")
print(EigenvectorsExample.1)
# assigns proper rownames names (as in the analyzed correlation matrix)
rownames(EigenvectorsExample.1) <- rownames(cor_Example.1)
print(EigenvectorsExample.1)
```
The PCA "vectors" are, in fact, the axes in an abstract mathematical space. The action of PCA can be explained as "rotation" of the analyzed matrix in order to identify the axis, along which the most matrix variation is presented. This will become Principal Component 1 ("PC1"). The next Principal Component should present the remaining matrix variation. Ideally, all of the remainder variation should be presented with Principal Component 2 ("PC2"). If this won't suffice, subsequent Principal Components can be calculated. Typically, the number of Principal Components equals the number of dimensions of the analyzed matrix. However, only the first few (ideally, two) of Principal Components should "pack" enough of dataset variation, so that the subsequent Principal Components can be discarded.

The PCA axes, described above, will be calculated as slopes. The slopes are, in fact, the rotation angles explained above. To determine the slopes, we will need to extract two coordinates. These coordinates are points in the aforementioned mathematical space that describe the rotation of the matrix required to yield the maximum (and then, the second maximum) proportion of matrix variation. This will be done using the eigenvector information from above. In particular, the slope of PC1 is quantified as the ratio between the first and second element of eigenvector "PC1". The slope of PC2 is quantified as the ratio between the first and second element of eigenvector "PC2". This rule applies to subsequent eigenvectors as well (if the analyzed matrix has more than two variables).
```{r}
print(EigenvectorsExample.1)
# transformation to a data frame simplifies the code
df_EigenvectorsExample.1 <- as.data.frame(EigenvectorsExample.1)
print(df_EigenvectorsExample.1)
# this calculates the slope for PC1
print(df_EigenvectorsExample.1$PC1[1])
print(df_EigenvectorsExample.1$PC1[2])
PC1_Example.1_slope <- df_EigenvectorsExample.1$PC1[1] / 
                        df_EigenvectorsExample.1$PC1[2]
print(PC1_Example.1_slope)
# this calculates the slope for PC2
print(df_EigenvectorsExample.1$PC2[1])
print(df_EigenvectorsExample.1$PC2[2])
PC2_Example.1_slope <- df_EigenvectorsExample.1$PC2[1] /
                        df_EigenvectorsExample.1$PC2[2]
print(PC2_Example.1_slope)
```
The next section of the script plots again the scaled matrix, with superimposed PC slopes. Important note: the plot aspect ratio should be appropriate (here: "asp = 1") for proper demonstration of PCs
```{r}
plot(df_Example.1_sc,
     asp = 1,
     pch = 21,
     xlim = c(-4, 4),
     ylim = c(-2, 2),
     xlab = "x",
     ylab = "y1",
     main = NA)
# this point identifies the center of the visualized matrix
points(x = mean(df_Example.1_sc$x_vect),
       y = mean(df_Example.1_sc$y1_vect), 
       col = "gray50", pch = 18, cex = 2)
# this draws a dark blue line showing how PC1 will go through the matrix
# the most matrix variation is represented among this PC
abline(a = 0, b = PC1_Example.1_slope, col = "navyblue", lty = 1, lwd = 1.5)
# this draws a red line showing how PC2 will go through the matrix
# (ideally, orthogonally to PC1)
abline(a = 0, b = PC2_Example.1_slope, col = "firebrick2", lty = 2, lwd = 1.5)
```

Please take a note that the data variation along the red axis (PC2) is much smaller than the variation along the blue axis (PC1).

The next section of the code will show how to procure Principal Components using the "prcomp()" function of base R. This function permits the use of the actual matrix. That is, there is no need for pre-calculation of a correlation matrix. This is because function "prcomp()" utilizes "svd()" (that is, singular value decomposition), which is another way to do the matrix decomposition.

Please note that all computation steps are done using matrices (here: "Example.1_sc"), whereas data visualization is most often done using data frames. There are specific reasons which will be addressed in subsequent classes
```{r}
prcomp_Example.1 <- prcomp(Example.1_sc,
                           scale = FALSE) # normalization ("scale()") is not
# necessary, as the matrix was already scaled; otherwise put "scale = TRUE"
print(prcomp_Example.1) 
# you should take a note how the output "Rotation (n x k) = (2 x 2):" produces 
# the values similar to the eigenvalue decomposition
summary(prcomp_Example.1)
# application of R function "summary()" to object "prcomp_Example.1" gives 
# a very useful piece of information: Cumulative Proportion. This Cumulative 
# Proportion puts a quantitative value to the proporation of the matrix 
# variation "packed" in respective PC's. Here, PC1 "packs" 0.925 (that is, 
# 92.5%) of the total variation, meaning that PC2 "packs" only the remaining 
# 7.5%
str(prcomp_Example.1) # this shows the structure of "prcomp_Example.1"
# "prcomp_Example.1$x" contains the data point coordinates in the aforementioned
# mathematical space
df_prcomp_Example.1 <- as.data.frame(prcomp_Example.1$x)
print(df_prcomp_Example.1)
```
This will plot the matrix with coordinates from PC1 and PC2.
```{r}
plot(df_prcomp_Example.1,
     asp = 1,
     pch = 21,
     xlim = c(-4, 4),
     ylim = c(-2, 2),
     xlab = "PC1",
     ylab = "PC2",
     main = NA)
points(x = mean(df_prcomp_Example.1$PC1),
       y = mean(df_prcomp_Example.1$PC2), 
       col = "gray50", pch = 18, cex = 2)
abline(a = 0, b = 0, col = "navyblue", lty = 1, lwd = 1.5)
abline(v = 0, col = "firebrick2", lty = 2, lwd = 1.5)
```

Please take a note that the distribution of data points looks very similar to the original matrix, but (1) rotated counterclockwise along PC2 axis and (2) then flipped to 180 degrees. This effect is typical for the use of "prcomp()". 